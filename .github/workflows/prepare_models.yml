name: Prepare models

on:
  workflow_dispatch:
    inputs:
      ref:
        description: "Бранч или тег, для которого нужно подготовить модели"
        default: main
        required: true
        type: string

permissions:
  contents: write
  pull-requests: write

concurrency:
  group: prepare-models-${{ github.ref }}
  cancel-in-progress: false

jobs:
  prepare:
    name: Prepare and publish models
    runs-on: ubuntu-latest
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    defaults:
      run:
        shell: bash
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.ref || github.ref }}

      - name: Disk space diagnostic
        run: |
          echo "Initial disk space:"
          df -h

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Enable swap
        run: |
          sudo fallocate -l 10G /swapfile
          sudo chmod 600 /swapfile
          sudo mkswap /swapfile
          sudo swapon /swapfile
          free -h

      - name: Install system dependencies and build NCNN tools
        run: |
          set -euxo pipefail
          sudo apt-get update
          APT_PACKAGES="git-lfs wget unzip ffmpeg libonnx-dev protobuf-compiler libprotobuf-dev cmake ninja-build build-essential lld"
          sudo apt-get install -y ${APT_PACKAGES}

          NCNN_REPO=https://github.com/Tencent/ncnn.git
          NCNN_COMMIT=56775de50990ab7f16627efdcf5529b49541206f
          git clone ${NCNN_REPO} ncnn
          git -C ncnn checkout ${NCNN_COMMIT}
          cmake -S ncnn -B ncnn/build -G Ninja \
            -DNCNN_VULKAN=OFF \
            -DNCNN_BUILD_TOOLS=ON \
            -DNCNN_BUILD_EXAMPLES=OFF \
            -DNCNN_BUILD_BENCHMARK=OFF
          cmake --build ncnn/build --target onnx2ncnn ncnnoptimize
          ls -l ncnn/build/tools
          sudo install -m 0755 ncnn/build/tools/onnx/onnx2ncnn /usr/local/bin/onnx2ncnn
          sudo install -m 0755 ncnn/build/tools/ncnnoptimize /usr/local/bin/ncnnoptimize
          sudo ldconfig
          if ! onnx2ncnn --help >/dev/null 2>&1; then
            echo "onnx2ncnn не поддерживает --help, выполняем холостой запуск"
            onnx2ncnn </dev/null >/dev/null 2>&1 || true
          fi
          if ! ncnnoptimize --help >/dev/null 2>&1; then
            echo "ncnnoptimize не поддерживает --help, выполняем холостой запуск"
            ncnnoptimize </dev/null >/dev/null 2>&1 || true
          fi
          rm -rf ncnn

      - name: Verify NCNN tools availability
        run: |
          set -euxo pipefail
          command -v onnx2ncnn
          onnx2ncnn </dev/null >/dev/null 2>&1 || true
          command -v ncnnoptimize
          ncnnoptimize </dev/null >/dev/null 2>&1 || true

      - name: Install Python dependencies (lean, CPU-only)
        env:
          PIP_NO_CACHE_DIR: "1"
        run: |
          set -euxo pipefail
          python3 -m pip install --upgrade pip
          python3 -m pip install --no-cache-dir \
            torch==2.9.0 \
            torchvision \
            --index-url https://download.pytorch.org/whl/cpu \
            --extra-index-url https://pypi.org/simple \
            numpy==1.26.4 onnx==1.16.2 onnxruntime==1.16.3 onnxsim==0.4.36 \
            onnx-graphsurgeon==0.5.8 einops==0.8.1 opencv-python-headless==4.11.0.86 \
            tqdm==4.67.1 pyyaml==6.0.3 scipy==1.13.1 scikit-image==0.22.0 \
            psutil==7.1.2 lmdb==1.7.5 addict==2.4.0
          python3 - <<'PY'
          import onnx
          print(f"ONNX {onnx.__version__} OK")
          PY
          python3 - <<'PY'
          import importlib.metadata as metadata
          packages = [
              "numpy",
              "torch",
              "torchvision",
              "onnx",
              "onnxruntime",
              "onnxsim",
              "onnx-graphsurgeon",
              "psutil",
              "einops",
              "opencv-python-headless",
              "tqdm",
              "pyyaml",
              "scipy",
              "scikit-image",
              "lmdb",
              "addict",
          ]
          for pkg in packages:
              try:
                  version = metadata.version(pkg)
              except metadata.PackageNotFoundError:
                  version = "не установлен"
              print(f"{pkg}: {version}")
          PY

      - name: Prepare model artifacts
        env:
          PYTHONPATH: "${{ github.workspace }}:$PYTHONPATH"
          OMP_NUM_THREADS: "1"
        run: |
          set -euxo pipefail
          bash scripts/prepare_models.sh

      - name: Configure Git user
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

      - name: Publish release
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euxo pipefail
          bash scripts/publish_models_release.sh

      - name: Unpack models to dist/models/
        run: |
          set -euxo pipefail
          mkdir -p dist/models
          for zip_file in dist/*.zip; do
            if [[ -f "$zip_file" ]]; then
              echo "Распаковка $zip_file..."
              unzip -o "$zip_file" -d dist/
            fi
          done
          ls -lah dist/models/

      - name: Ensure SHA256SUMS.txt (deterministic)
        run: |
          if [[ ! -f dist/SHA256SUMS.txt ]]; then
            cd dist
            find models -type f -print0 | sort -z | xargs -0 -I{} sh -c 'sha256sum "{}"' > SHA256SUMS.txt
          fi

      - name: Generate models.lock.json
        env:
          MODELS_DIST: dist
          MODELS_CONTRACT_VERSION: v1.4.1
        run: python tools/gen_models_lock.py

      - name: Validate models.lock.json
        run: |
          python - <<'PY'
          import json, pathlib, hashlib, sys
          d = pathlib.Path("dist")
          lock = json.loads((d/"models.lock.json").read_text())
          ok = True
          assert isinstance(lock.get("entries"), list), "entries must be list"
          assert lock.get("api_contract_version") == "v1.4.1", "api_contract_version must be v1.4.1"
          for e in lock["entries"]:
            if e["backend"] == "METADATA":
              v = e.get("jq_sha256sum", "")
              if not (isinstance(v, str) and (len(v) == 64 or len(v) == 0)):
                print("Bad METADATA.jq_sha256sum")
                ok = False
              continue
            if e["backend"] not in ("ncnn", "tflite"):
              print("Unsupported backend:", e["backend"])
              ok = False
            for f in e.get("files", []):
              p = d / f["path"]
              if not p.exists():
                print("Missing file:", p)
                ok = False
                continue
              h = hashlib.sha256(p.read_bytes()).hexdigest()
              if h != f["sha256"]:
                print("SHA mismatch:", p, h, "!=", f["sha256"])
                ok = False
          sys.exit(1 if not ok else 0)
          PY

      - name: Sync lock to repo root (single source of truth)
        run: |
          cp -f dist/models.lock.json models.lock.json
          git diff --quiet models.lock.json || echo "models.lock.json updated"

      - name: Create PR with updated models.lock.json
        uses: peter-evans/create-pull-request@v6
        with:
          add-paths: models.lock.json
          commit-message: "chore(models): update models.lock.json from Prepare models"
          title: "chore(models): update models.lock.json"
          branch: chore/update-models-lock
          delete-branch: true

      - name: Upload artifact (models-v1)
        uses: actions/upload-artifact@v4
        with:
          name: models-v1
          path: |
            dist/models/**
            dist/SHA256SUMS.txt
            dist/models.lock.json
          if-no-files-found: error
          retention-days: 14
